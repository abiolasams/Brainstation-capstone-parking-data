# -*- coding: utf-8 -*-
"""Samuel.Abiola.Final_capstone_submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nminOssHkp4E61B2nDi8H8f0YJGySP6x

### Import all the required libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import html5lib
import lxml
import requests
import keras
from bs4 import BeautifulSoup
import ast
from pandas.io.json import json_normalize

from scipy import stats
from scipy.stats import norm, skew #for some statistics

"""##### loading the parking data obtained from kaggle"""

df=pd.read_csv('Searching_for_parking_NA.csv')

df.head()

"""###### Cleaning the original kaggle data since SearchingByHour and HourlyDistribution are in json object formats"""

#converting null to 0 for the selected columns
df['SearchingByHour']=df['SearchingByHour'].str.replace("null","0")
df['HourlyDistribution']=df['HourlyDistribution'].str.replace("null","0")
df['CirclingDistribution']=df['CirclingDistribution'].str.replace("null","0")

#converting the columns to string
df['SearchingByHour']=df['SearchingByHour'].apply(ast.literal_eval)
df['HourlyDistribution']=df['HourlyDistribution'].apply(ast.literal_eval)
df['CirclingDistribution']=df['CirclingDistribution'].apply(ast.literal_eval)

#putting the JSON object found in the HourlyDistribution column and the searchingbyhour column into a workable format for pandas
dict_cols = pd.DataFrame(json_normalize(df["SearchingByHour"]).stack()).reset_index(level=1)
dict_cols.columns = ['hours_1','search_values']

dict_cols2 = pd.DataFrame(json_normalize(df["HourlyDistribution"]).stack()).reset_index(level=1)
dict_cols2.columns = ['hours','hd_values']

dict_cols.head()

dict_cols2.head()

dict_cols3 = pd.DataFrame(json_normalize(df["CirclingDistribution"]).stack()).reset_index(level=1)
dict_cols3.columns = ['Geohash','circling_values']

dict_cols3.head(20)

#joining the hourlydistribution and searchingbyhour columns to the original df database
search = pd.concat([df, dict_cols], axis=1)
search=pd.concat([search,dict_cols2],axis=1)

search.head()

"""### Webscraping from wikipedia"""

table = pd.read_html('https://en.wikipedia.org/wiki/List_of_North_American_cities_by_population')
tabs=table[0]

tabs.head()

"""##### Cleaning the webscraped data's population column"""

#removing the references from the population column
new = tabs['Population'].str.split("[", n = 1, expand = True) 
tabs["pop"]= new[0] 

#removing commas in the population column
tabs['pop']=tabs['pop'].apply(lambda x: float(x.split()[0].replace(",","")))

#for further merges or join New York city has to be edited 
tabs["City"]= tabs["City"].replace('New York City', "New York")

tabs.head()

"""### Webscraping GDP data """

sable = pd.read_html('https://en.wikipedia.org/wiki/List_of_cities_by_GDP_(PPP)_per_capita')
sabs=sable[0]
sabs.columns=['Metro_ID','City','GDP_per_capita']

sabs.head()

"""##### Cleaning the GDP data"""

# removing references or brackets from the column
new = sabs['City'].str.split("(", n = 1, expand = True) 
sabs["City"]= new[0] 

#removing white spaces from the end of the data
sabs['City']=sabs['City'].str.rstrip()

sabs.head()

"""##### merging the kaggle data and the webscraped data into one dataframe"""

#joining the cleaned kaggle columns to the population data
allin=pd.merge(search, tabs, how='left', on='City')
allin=allin.drop(['Image','Country_y','Population','Year'],axis=1)
allin=allin.dropna()

#joining the kaggle + population columns to the GDP per capita data
allin2=pd.merge(allin, sabs, how='left', left_on='City', right_on='City')
allin2=allin2.dropna()

allin2=allin2.drop(['Version','Latitude_SW','Longitude_SW','Latitude_NE','Longitude_NE'],axis=1)
allin2=allin2.drop(['PercentOther','UpdateDate','Metro_ID','GeohashBounds','Location'],axis=1)
allin2=allin2.drop(['Geohash','ISO_3166_2','County','CirclingDistribution','HourlyDistribution','SearchingByHour'],axis=1)
allin2=allin2.drop(['hours_1','Unnamed: 0','Latitude','Longitude'],axis=1)

allin2.head()

"""### Exploratory Data Analysis

##### Boxplot for the average time to park for each city 

##### New York and Boston have the highest average time to park times at about 7 mins
##### The lowest time is Vancouver at about 2 mins
"""

sns.catplot(y = "AvgTimeToPark", x = "City", data = allin.sort_values("AvgTimeToPark", ascending = False), kind="boxen", height = 4, aspect = 3)

#g.set_xticklabels(rotation=90)
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()
plt.savefig('boxplotforcitiesavgpark.jpg')

"""#### Distribution plot for the average time to park for the data set

#### the distribution plot is fairly normal.
"""

#Check the distribution to see the normalized data
sns.distplot(allin['AvgTimeToPark'] , fit=norm);

"""#### Correlation heat map for the correlation plot for the features used in the model"""

# Create correlation matrix
corr_matrix = allin2.corr()

fig = plt.figure(figsize= (10,10))
sns.heatmap(corr_matrix, annot=True, vmax=1, cmap='viridis', square=False)

"""### Feature Engineering

Data Cleaning
"""

#assessing all the columns in the data that have missing data 
forever=round(allin2.isnull().mean()*100,2)

forever

"""thus no missing data to clean up

#### Dealing with Skewed data
"""

# Fetch all numeric features
numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numeric = []
for i in allin2.columns:
    if allin2[i].dtype in numeric_dtypes:
        numeric.append(i)

# Create box plots for all numeric features
#sns.set_style("white")
f, ax = plt.subplots(figsize=(10, 6))
ax.set_xscale("log")
ax = sns.boxplot(data=allin2[numeric] , orient="h", palette="Set1")
ax.xaxis.grid(False)
ax.set(ylabel="Feature names")
ax.set(xlabel="Numeric values")
ax.set(title="Numeric Distribution of Features")
#sns.despine(trim=True, left=True)

# Find skewed numerical features
skew_features = allin2[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)

high_skew = skew_features[skew_features > 0.5]
skew_index = high_skew.index

print("There are {} numerical features with Skew > 0.5 :".format(high_skew.shape[0]))
skewness = pd.DataFrame({'Skew' :high_skew})
skew_features.head(10)

"""using box cox transformation to normalize the data and address the skew"""

# Stats
from scipy.stats import skew, norm
from scipy.special import boxcox1p
from scipy.stats import boxcox_normmax

# Normalize skewed features
for i in skew_index:
    allin2[i] = boxcox1p(allin2[i], boxcox_normmax(allin2[i] + 1))

# Let's make sure we handled all the skewed values
sns.set_style("white")
f, ax = plt.subplots(figsize=(8, 7))
ax.set_xscale("log")
ax = sns.boxplot(data=allin2[skew_index] , orient="h", palette="Set1")
ax.xaxis.grid(False)
ax.set(ylabel="Feature names")
ax.set(xlabel="Numeric values")
ax.set(title="Numeric Distribution of Features")
sns.despine(trim=True, left=True)

"""#### Utilizing dummy variables for the categorical variables"""

allin2 = pd.get_dummies(allin2).reset_index(drop=True)
allin2.head()

"""## Regression Analysis

Three types of regression are chosen for the analysis: Linear Regression, Decision Tree Regression and Random Forest Regression

### Linear Regression
"""

from sklearn.linear_model import Ridge, LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score

X=allin2.drop(['AvgTimeToPark'],axis=1)
y=allin2.AvgTimeToPark

# Create a train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

#Transform data
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""#### Default linear regression"""

model = LinearRegression()
model.fit(X_train, y_train)

print(f"R^2 score on training set: {model.score(X_train, y_train):0.3f}")
print(f"R^2 score on test set: {model.score(X_test, y_test):0.3f}")

#defining the coefficients of the dataset
coeff_df = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])

coeff1=coeff_df.sort_values(by=['Coefficient'], ascending=False)

"""#### the most postive coefficients"""

coeff1.head()

"""#### the most negative coefficients"""

coeff1.tail()

y_pred = model.predict(X_test)

from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""### Other models

Key features of the model training process:

Cross Validation: Using 12-fold cross-validation

Models: On each run of cross-validation I fit 4 models (ridge, random forest, xgboost, lightgbm regressors)

Stacking: In addition, I trained a meta StackingCVRegressor optimized using xgboost

Blending: All models trained will overfit the data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold, cross_val_score

# Setup cross validation folds
kf = KFold(n_splits=12, random_state=42, shuffle=True)

# Define error metrics
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y_test, y_pred))

def cv_rmse(model, X=X):
    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring="neg_mean_squared_error", cv=kf))
    return (rmse)

"""Models"""

# Models
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.svm import SVR
from mlxtend.regressor import StackingCVRegressor
import lightgbm as lgb
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import scale
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA

# Light Gradient Boosting Regressor
lightgbm = LGBMRegressor(objective='regression', 
                       num_leaves=6,
                       learning_rate=0.01, 
                       n_estimators=7000,
                       max_bin=200, 
                       bagging_fraction=0.8,
                       bagging_freq=4, 
                       bagging_seed=8,
                       feature_fraction=0.2,
                       feature_fraction_seed=8,
                       min_sum_hessian_in_leaf = 11,
                       verbose=-1,
                       random_state=42)

# XGBoost Regressor
xgboost = XGBRegressor(learning_rate=0.01,
                       n_estimators=6000,
                       max_depth=4,
                       min_child_weight=0,
                       gamma=0.6,
                       subsample=0.7,
                       colsample_bytree=0.7,
                       objective='reg:linear',
                       nthread=-1,
                       scale_pos_weight=1,
                       seed=27,
                       reg_alpha=0.00006,
                       random_state=42)

# Ridge Regressor
ridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]
ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))
  

# Random Forest Regressor
rf = RandomForestRegressor(n_estimators=1200,
                          max_depth=15,
                          min_samples_split=5,
                          min_samples_leaf=5,
                          max_features=None,
                          oob_score=True,
                          random_state=42)

# Stack up all the models above, optimized using xgboost
stack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, ridge, rf),
                                meta_regressor=xgboost,
                                use_features_in_secondary=True)

"""Training the models"""

scores = {}

score = cv_rmse(lightgbm)
print("lightgbm: {:.4f} ({:.4f})".format(score.mean(), score.std()))
scores['lgb'] = (score.mean(), score.std())

score = cv_rmse(xgboost)
print("xgboost: {:.4f} ({:.4f})".format(score.mean(), score.std()))
scores['xgb'] = (score.mean(), score.std())

score = cv_rmse(ridge)
print("ridge: {:.4f} ({:.4f})".format(score.mean(), score.std()))
scores['ridge'] = (score.mean(), score.std())

score = cv_rmse(rf)
print("rf: {:.4f} ({:.4f})".format(score.mean(), score.std()))
scores['rf'] = (score.mean(), score.std())

"""Fitting the models """

print('stack_gen')
stack_gen_model = stack_gen.fit(np.array(X_train), np.array(y_train))

print('lightgbm')
lgb_model_full_data = lightgbm.fit(X_train, y_train)

print('xgboost')
xgb_model_full_data = xgboost.fit(X_train, y_train)

print('Ridge')
ridge_model_full_data = ridge.fit(X_train, y_train)

print('RandomForest')
rf_model_full_data = rf.fit(X_train, y_train)

"""Blended models and predictions"""

y_preds = ridge_model_full_data.predict(X_test)
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_preds))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_preds)))

y_preds = xgb_model_full_data.predict(X_test)
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_preds))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_preds)))

y_preds = rf_model_full_data.predict(X_test)
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_preds))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_preds)))

y_preds = lgb_model_full_data .predict(X_test)
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_preds))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_preds)))

# Blend models in order to make the final predictions more robust to overfitting
def blended_predictions(X):
    return ((0.1 * ridge_model_full_data.predict(X)) + \
            (0.1 * xgb_model_full_data.predict(X)) + \
            (0.1 * lgb_model_full_data.predict(X)) + \
            (0.05 * rf_model_full_data.predict(X)) + \
            (0.35 * stack_gen_model.predict(np.array(X))))

# Get final precitions from the blended model
blended_score = rmsle(y_test, blended_predictions(X_test))
scores['blended'] = (blended_score, 0)
print('RMSLE score on train data:')
print(blended_score)

# Plot the predictions for each model
sns.set_style("white")
fig = plt.figure(figsize=(24, 12))

ax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])
for i, score in enumerate(scores.values()):
    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')

plt.ylabel('Score (RMSE)', size=20, labelpad=12.5)
plt.xlabel('Model', size=20, labelpad=12.5)
plt.tick_params(axis='x', labelsize=13.5)
plt.tick_params(axis='y', labelsize=12.5)

plt.title('Scores of Models', size=20)

plt.show()

"""### Decision Tree Regression

#### Initial Decision tree regressor
"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error
from scipy.stats import randint

DTmodel = DecisionTreeRegressor()
DTmodel.fit(X_train, y_train)

print(f"R^2 score on training set: {DTmodel.score(X_train, y_train):0.3f}")
print(f"R^2 score on test set: {DTmodel.score(X_test, y_test):0.3f}")

"""#### Optimized decision tree regression"""

# Setup the parameters and distributions to sample from: param_dist
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ['mse','mae']}

# Instantiate a Decision Tree classifier: tree
tree = DecisionTreeRegressor()

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fit it to the data
tree_cv.fit(X_train, y_train)

# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))

y_preds = tree_cv.predict(X_test)

#Computing r2_score
from sklearn.metrics import r2_score
r2_score(y_test,y_preds)

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_preds))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_preds))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_preds)))

"""#### Feature importance for the Decision tree model"""

importances = DTmodel.feature_importances_
feat_names = X.columns
tree_result = pd.DataFrame({'feature': feat_names, 'importance': importances})
tree_result.sort_values(by='importance',ascending=False)[:15].plot(x='feature', y='importance', kind='bar',color='blue')
plt.savefig('decisontreeimportantfeatures.png')

"""### Random Forest Regression"""

from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn import preprocessing, svm
from sklearn.model_selection import cross_validate

#fitting the initial random forest regressor to the training data
clf = RandomForestRegressor()
clf.fit(X_train, y_train)

r2_score = clf.score(X_test, y_test)

print(r2_score)

y_preda = clf.predict(X_test)

#Computing r2_score
from sklearn.metrics import r2_score
r2_score(y_test,y_preda)

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""#### Another Random Forest Regressor using RandomizedsearchCV"""

random_grid = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ['mse','mae']}

# Use the random grid to search for best hyperparameters
# First create the base model to tune
clf = RandomForestRegressor()
# Random search of parameters, using 3 fold cross validation,
# search across 100 different combinations, and use all available cores
clf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)

# Fit the random search model
clf_random.fit(X_train, y_train)
clf_random.best_params_

ans = clf_random.predict(X_test)

def evaluate(model, test_features, test_labels):
    predictions = model.predict(test_features)
    errors = abs(predictions - test_labels)
    mape = 100 * np.mean(errors / test_labels)
    accuracy = 100 - mape
    print('Model Performance')
    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))
    print('Accuracy = {:0.2f}%.'.format(accuracy))

    return accuracy

best_random = clf_random.best_estimator_
random_accuracy = evaluate(best_random,X_test, y_test)

from sklearn.metrics import r2_score
r2_score(y_test, ans)

from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, ans))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, ans))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ans)))

"""#### Feature importance based on the un optimized model"""

importances = clf.feature_importances_
feat_names = X.columns
tree_result = pd.DataFrame({'feature': feat_names, 'importance': importances})
tree_result.sort_values(by='importance',ascending=False)[:15].plot(x='feature', y='importance', kind='bar',color='blue')
plt.savefig('decisontreeimportantfeatures.png')